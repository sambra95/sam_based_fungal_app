{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10837f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xz/wnmpfwfd79bbx_lqntg_fwgc0000gp/T/ipykernel_8555/2258350492.py:173: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data] Found 88 image/mask pairs.\n",
      "step 000000 | loss 0.3114 | mean IoU 0.0047\n",
      "step 000010 | loss 0.0151 | mean IoU 0.0708\n",
      "step 000020 | loss 0.0087 | mean IoU 0.1349\n",
      "step 000030 | loss 0.0078 | mean IoU 0.1950\n",
      "step 000040 | loss 0.0041 | mean IoU 0.2498\n",
      "step 000050 | loss 0.0046 | mean IoU 0.3021\n",
      "step 000060 | loss 0.0080 | mean IoU 0.3435\n",
      "step 000070 | loss 0.0084 | mean IoU 0.3870\n",
      "step 000080 | loss 0.0044 | mean IoU 0.4255\n",
      "step 000090 | loss 0.0035 | mean IoU 0.4592\n",
      "step 000100 | loss 0.0056 | mean IoU 0.4938\n",
      "step 000110 | loss 0.0067 | mean IoU 0.5225\n",
      "step 000120 | loss 0.0054 | mean IoU 0.5443\n",
      "step 000130 | loss 0.0100 | mean IoU 0.5625\n",
      "step 000140 | loss 0.0051 | mean IoU 0.5867\n",
      "step 000150 | loss 0.0073 | mean IoU 0.6092\n",
      "step 000160 | loss 0.0073 | mean IoU 0.6286\n",
      "step 000170 | loss 0.0097 | mean IoU 0.6414\n",
      "step 000180 | loss 0.0030 | mean IoU 0.6562\n",
      "step 000190 | loss 0.0055 | mean IoU 0.6696\n",
      "step 000200 | loss 0.0032 | mean IoU 0.6826\n",
      "step 000210 | loss 0.0047 | mean IoU 0.6881\n",
      "step 000220 | loss 0.0058 | mean IoU 0.6976\n",
      "step 000230 | loss 0.0038 | mean IoU 0.7078\n",
      "step 000240 | loss 0.0035 | mean IoU 0.7189\n",
      "step 000250 | loss 0.0036 | mean IoU 0.7256\n",
      "step 000260 | loss 0.0027 | mean IoU 0.7350\n",
      "step 000270 | loss 0.0058 | mean IoU 0.7411\n",
      "step 000280 | loss 0.0036 | mean IoU 0.7472\n",
      "step 000290 | loss 0.0058 | mean IoU 0.7515\n",
      "step 000300 | loss 0.0091 | mean IoU 0.7581\n",
      "step 000310 | loss 0.0015 | mean IoU 0.7616\n",
      "step 000320 | loss 0.0055 | mean IoU 0.7654\n",
      "step 000330 | loss 0.0051 | mean IoU 0.7644\n",
      "step 000340 | loss 0.0037 | mean IoU 0.7681\n",
      "step 000350 | loss 0.0063 | mean IoU 0.7719\n",
      "step 000360 | loss 0.0041 | mean IoU 0.7739\n",
      "step 000370 | loss 0.0031 | mean IoU 0.7763\n",
      "step 000380 | loss 0.0042 | mean IoU 0.7771\n",
      "step 000390 | loss 0.0023 | mean IoU 0.7806\n",
      "step 000400 | loss 0.0025 | mean IoU 0.7856\n",
      "step 000410 | loss 0.0065 | mean IoU 0.7847\n",
      "step 000420 | loss 0.0067 | mean IoU 0.7878\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 181\u001b[0m\n\u001b[1;32m    179\u001b[0m mean_iou \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m itr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ITERS):\n\u001b[0;32m--> 181\u001b[0m     images, masks_np, points_np, labels_np \u001b[38;5;241m=\u001b[39m \u001b[43mread_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     images \u001b[38;5;241m=\u001b[39m [increase_contrast_pil(image, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m masks_np\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[1], line 151\u001b[0m, in \u001b[0;36mread_batch\u001b[0;34m(pairs, batch_size)\u001b[0m\n\u001b[1;32m    149\u001b[0m images, masks, pts \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m--> 151\u001b[0m     I, M, P \u001b[38;5;241m=\u001b[39m \u001b[43msample_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(I)\n\u001b[1;32m    153\u001b[0m     masks\u001b[38;5;241m.\u001b[39mappend(M)\n",
      "Cell \u001b[0;32mIn[1], line 123\u001b[0m, in \u001b[0;36msample_single\u001b[0;34m(pairs)\u001b[0m\n\u001b[1;32m    120\u001b[0m Img \u001b[38;5;241m=\u001b[39m Img[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# load masks stack (N,H,W); choose one non-empty slice\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m ms \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_load_npy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpy_fp\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (N,H,W) uint8\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ms\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m ms\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# no masks; resample\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sample_single(pairs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # edited from this repository fine-tune-train_segment_anything_2_in_60_lines_of_code\n",
    "# fine_tune_sam2_from_npymasks.py\n",
    "# Fine-tune SAM2 from image/mask pairs saved by your app as <stem>_mask.npy\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "from contextlib import nullcontext\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "TRAIN_DIR  = \"/Users/sambra/Desktop/training_images/\"  # your folder\n",
    "IMAGE_EXTS = {\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\"}\n",
    "TARGET_SIZE = 512  # resize long side to <=1024 and pad to 1024x1024\n",
    "\n",
    "# Model/opt\n",
    "CKPT_PATH = \"/Users/sambra/Documents/GitHub/sam2_clone/checkpoints/sam2.1_hiera_tiny.pt\"\n",
    "CFG_PATH  = \"/configs/sam2.1/sam2.1_hiera_t.yaml\"\n",
    "DEVICE    = (\"cuda\" if torch.cuda.is_available()\n",
    "             else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "LR        = 1e-5\n",
    "WEIGHT_DECAY = 4e-5\n",
    "ITERS     = 5000\n",
    "BATCH_SIZE = 8  # keep 1 for simplicity with SAM2 prompt/feature plumbing\n",
    "SAVE_EVERY = 100\n",
    "OUT_WEIGHTS = \"/Users/sambra/Documents/GitHub/sam2_clone/checkpoints/sam2.1_hiera_l_finetuned.pt\"\n",
    "\n",
    "# -----------------------------------------\n",
    "\n",
    "def increase_contrast_pil(img_in, factor=5):\n",
    "    img_pil = img_in if isinstance(img_in, Image.Image) else Image.fromarray(img_in)\n",
    "    return np.array(ImageEnhance.Contrast(img_pil.convert(\"RGB\")).enhance(factor), dtype=np.uint8)\n",
    "\n",
    "\n",
    "def _safe_load_npy(fp: str) -> np.ndarray:\n",
    "    \"\"\"Load .npy (prefer no pickle; fallback if needed). Expect (N,H,W).\"\"\"\n",
    "    try:\n",
    "        arr = np.load(fp, allow_pickle=False)\n",
    "    except ValueError as e:\n",
    "        if \"pickled\" not in str(e).lower():\n",
    "            raise\n",
    "        # Only load trusted files with pickle!\n",
    "        arr = np.load(fp, allow_pickle=True)\n",
    "        print(f\"[warn] Loaded with allow_pickle=True: {fp}\")\n",
    "    arr = np.asarray(arr)\n",
    "    if arr.ndim == 2:  # (H,W) -> (1,H,W)\n",
    "        arr = arr[None, ...]\n",
    "    return (arr > 0).astype(np.uint8)\n",
    "\n",
    "\n",
    "def collect_pairs(root: str):\n",
    "    \"\"\"Return list of (image_path, mask_npy_path) where *_mask.npy exists.\"\"\"\n",
    "    pairs = []\n",
    "    for name in os.listdir(root):\n",
    "        p = Path(root) / name\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        if p.suffix.lower() in IMAGE_EXTS:\n",
    "            stem = p.stem\n",
    "            mask_fp = Path(root) / f\"{stem}_mask.npy\"\n",
    "            if mask_fp.exists():\n",
    "                pairs.append((str(p), str(mask_fp)))\n",
    "    if not pairs:\n",
    "        raise RuntimeError(f\"No image/mask pairs found in {root}. \"\n",
    "                           f\"Expect files like image.png and image_mask.npy.\")\n",
    "    print(f\"[data] Found {len(pairs)} image/mask pairs.\")\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def resize_and_pad_1024(img: np.ndarray, mask: np.ndarray):\n",
    "    \"\"\"\n",
    "    Resize image (H,W,3) and mask (H,W) with same ratio (<=1024) then pad to (1024,1024).\n",
    "    \"\"\"\n",
    "    H, W = img.shape[:2]\n",
    "    r = min(TARGET_SIZE / W, TARGET_SIZE / H)\n",
    "    new_w, new_h = int(W * r), int(H * r)\n",
    "\n",
    "    img_r = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "    msk_r = cv2.resize(mask, (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # pad to TARGET_SIZE square\n",
    "    pad_bottom = TARGET_SIZE - new_h\n",
    "    pad_right  = TARGET_SIZE - new_w\n",
    "\n",
    "    if pad_bottom > 0:\n",
    "        img_r = np.concatenate(\n",
    "            [img_r, np.zeros([pad_bottom, new_w, 3], dtype=np.uint8)], axis=0\n",
    "        )\n",
    "        msk_r = np.concatenate(\n",
    "            [msk_r, np.zeros([pad_bottom, new_w], dtype=np.uint8)], axis=0\n",
    "        )\n",
    "    if pad_right > 0:\n",
    "        img_r = np.concatenate(\n",
    "            [img_r, np.zeros([TARGET_SIZE, pad_right, 3], dtype=np.uint8)], axis=1\n",
    "        )\n",
    "        msk_r = np.concatenate(\n",
    "            [msk_r, np.zeros([TARGET_SIZE, pad_right], dtype=np.uint8)], axis=1\n",
    "        )\n",
    "    return img_r, msk_r\n",
    "\n",
    "\n",
    "def sample_single(pairs):\n",
    "    \"\"\"\n",
    "    Pick one image/mask pair, choose a random instance from the stack,\n",
    "    return (Img_1024x1024, mask_1024x1024, [[x,y]]) with a positive point.\n",
    "    \"\"\"\n",
    "    # pick random pair\n",
    "    img_fp, npy_fp = pairs[np.random.randint(len(pairs))]\n",
    "\n",
    "    # load image (BGR->RGB)\n",
    "    Img = cv2.imread(img_fp)\n",
    "    if Img is None:\n",
    "        raise RuntimeError(f\"Failed to read image: {img_fp}\")\n",
    "    Img = Img[..., ::-1]\n",
    "\n",
    "    # load masks stack (N,H,W); choose one non-empty slice\n",
    "    ms = _safe_load_npy(npy_fp)  # (N,H,W) uint8\n",
    "    if ms.size == 0 or ms.shape[0] == 0:\n",
    "        # no masks; resample\n",
    "        return sample_single(pairs)\n",
    "\n",
    "    # pick random non-empty mask\n",
    "    non_empty = [i for i in range(ms.shape[0]) if ms[i].any()]\n",
    "    if not non_empty:\n",
    "        return sample_single(pairs)\n",
    "    ind = non_empty[np.random.randint(len(non_empty))]\n",
    "    mask = (ms[ind] > 0).astype(np.uint8)\n",
    "\n",
    "    # resize & pad\n",
    "    Img, mask = resize_and_pad_1024(Img, mask)\n",
    "\n",
    "    # positive point inside mask\n",
    "    coords = np.argwhere(mask > 0)\n",
    "    if coords.size == 0:\n",
    "        return sample_single(pairs)\n",
    "    yx = coords[np.random.randint(len(coords))]\n",
    "    # SAM expects [[x,y]]\n",
    "    input_point = [[int(yx[1]), int(yx[0])]]\n",
    "    return Img, mask, input_point\n",
    "\n",
    "\n",
    "def read_batch(pairs, batch_size=BATCH_SIZE):\n",
    "    images, masks, pts = [], [], []\n",
    "    for _ in range(batch_size):\n",
    "        I, M, P = sample_single(pairs)\n",
    "        images.append(I)\n",
    "        masks.append(M)\n",
    "        pts.append(P)\n",
    "    # labels: 1 for positive points\n",
    "    labels = np.ones([batch_size, 1], dtype=np.int64)\n",
    "    masks = np.asarray(masks, dtype=np.uint8)  # (B, H, W)\n",
    "    pts = np.asarray(pts, dtype=np.int64)      # (B, 1, 2)\n",
    "    return images, masks, pts, labels\n",
    "\n",
    "\n",
    "# ---------------- Build / Prepare model ----------------\n",
    "sam = build_sam2(CFG_PATH, CKPT_PATH, device=DEVICE)\n",
    "predictor = SAM2ImagePredictor(sam)\n",
    "\n",
    "# trainable\n",
    "predictor.model.sam_mask_decoder.train(True)\n",
    "predictor.model.sam_prompt_encoder.train(True)\n",
    "predictor.model.image_encoder.train(False)  # NOTE: if the repo froze grads with no_grad(), remove those\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=predictor.model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "use_amp = (DEVICE == \"cuda\")\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=use_amp)\n",
    "\n",
    "# --------------- Data -------------------\n",
    "pairs = collect_pairs(TRAIN_DIR)\n",
    "\n",
    "# --------------- Train loop -------------\n",
    "mean_iou = 0.0\n",
    "for itr in range(ITERS):\n",
    "    images, masks_np, points_np, labels_np = read_batch(pairs, batch_size=BATCH_SIZE)\n",
    "    images = [increase_contrast_pil(image, factor=5) for image in images]\n",
    "    if masks_np.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    # Single-sample forward for stability with predictor internals\n",
    "    # (keeps close to the original 60-line recipe)\n",
    "    total_loss = 0.0\n",
    "    total_iou  = 0.0\n",
    "\n",
    "    for b in range(len(images)):\n",
    "        with (torch.autocast(\"cuda\", dtype=torch.bfloat16) if use_amp else nullcontext()):\n",
    "            # Encode image\n",
    "            predictor.set_image(images[b])  # (H,W,3) uint8\n",
    "\n",
    "            # Prepare prompts\n",
    "            mask_input, unnorm_coords, labels_t, _ = predictor._prep_prompts(\n",
    "                points_np[b:b+1], labels_np[b:b+1], box=None, mask_logits=None, normalize_coords=True\n",
    "            )\n",
    "            sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(\n",
    "                points=(unnorm_coords, labels_t), boxes=None, masks=None\n",
    "            )\n",
    "\n",
    "            # Decode masks\n",
    "            # features prepared by set_image()\n",
    "            high_res_features = predictor._features[\"high_res_feats\"]  # list of tensors\n",
    "            low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n",
    "                image_embeddings=predictor._features[\"image_embed\"],\n",
    "                image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "                sparse_prompt_embeddings=sparse_embeddings,\n",
    "                dense_prompt_embeddings=dense_embeddings,\n",
    "                multimask_output=True,\n",
    "                repeat_image=True,\n",
    "                high_res_features=high_res_features,\n",
    "            )\n",
    "            # Upscale to original (post-transform) resolution\n",
    "            prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])\n",
    "\n",
    "            # Losses\n",
    "            gt_mask = torch.tensor(masks_np[b:b+1].astype(np.float32), device=DEVICE)  # (1,H,W)\n",
    "            pred_prob = torch.sigmoid(prd_masks[:, 0])  # (1,H,W)\n",
    "\n",
    "            # BCE\n",
    "            seg_loss = (-gt_mask * torch.log(pred_prob + 1e-5)\n",
    "                        - (1 - gt_mask) * torch.log(1 - pred_prob + 1e-5)).mean()\n",
    "\n",
    "            # IoU score loss\n",
    "            inter = (gt_mask * (pred_prob > 0.5)).sum((1, 2))\n",
    "            union = gt_mask.sum((1, 2)) + (pred_prob > 0.5).sum((1, 2)) - inter\n",
    "            iou = inter / (union + 1e-6)\n",
    "            score_loss = torch.abs(prd_scores[:, 0].to(DEVICE) - iou).mean()\n",
    "\n",
    "            loss = seg_loss + 0.05 * score_loss\n",
    "\n",
    "        predictor.model.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += float(loss.detach().cpu())\n",
    "        total_iou  += float(iou.detach().cpu().mean())\n",
    "\n",
    "    mean_iou = (0.99 * mean_iou) + (0.01 * (total_iou / len(images)))\n",
    "    if itr % 10 == 0:\n",
    "        print(f\"step {itr:06d} | loss {total_loss/len(images):.4f} | mean IoU {mean_iou:.4f}\")\n",
    "\n",
    "    if itr % SAVE_EVERY == 0 and itr > 0:\n",
    "        #torch.save(predictor.model.state_dict(), OUT_WEIGHTS)\n",
    "        torch.save({\"model\": predictor.model.state_dict()}, OUT_WEIGHTS)\n",
    "\n",
    "# final save\n",
    "torch.save({\"model\": predictor.model.state_dict()}, OUT_WEIGHTS)\n",
    "print(\"Saved:\", OUT_WEIGHTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e49bc13",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SAM2AutomaticMaskGenerator' from 'sam2' (/Users/sambra/miniforge3/envs/sam2_env/lib/python3.10/site-packages/sam2/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msam2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SAM2AutomaticMaskGenerator\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# --- New: utilities for multi-seed training ---\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_pair\u001b[39m(img_fp: \u001b[38;5;28mstr\u001b[39m, mask_fp: \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SAM2AutomaticMaskGenerator' from 'sam2' (/Users/sambra/miniforge3/envs/sam2_env/lib/python3.10/site-packages/sam2/__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from contextlib import nullcontext\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sam2 import SAM2AutomaticMaskGenerator\n",
    "\n",
    "# --- New: utilities for multi-seed training ---\n",
    "\n",
    "def load_pair(img_fp: str, mask_fp: str):\n",
    "    \"\"\"Return RGB image (H,W,3) uint8 and masks (N,H,W) uint8.\"\"\"\n",
    "    import cv2\n",
    "    img = cv2.imread(img_fp)\n",
    "    if img is None:\n",
    "        raise RuntimeError(f\"Failed to read image: {img_fp}\")\n",
    "    img = img[..., ::-1]  # BGR->RGB\n",
    "    ms = _safe_load_npy(mask_fp)  # (N,H,W) uint8 0/1\n",
    "    return img, ms\n",
    "\n",
    "def resize_and_pad_square(img, masks, target=TARGET_SIZE):\n",
    "    \"\"\"Resize long side to target and pad to (target,target). Masks: (N,H,W).\"\"\"\n",
    "    import cv2\n",
    "    H, W = img.shape[:2]\n",
    "    r = min(target / W, target / H)\n",
    "    new_w, new_h = int(W * r), int(H * r)\n",
    "    img_r = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "    masks_r = np.stack(\n",
    "        [cv2.resize(m.astype(np.uint8), (new_w, new_h), interpolation=cv2.INTER_NEAREST) for m in masks],\n",
    "        axis=0\n",
    "    ) if masks.size else masks\n",
    "\n",
    "    # pad to square\n",
    "    pad_b, pad_r = target - new_h, target - new_w\n",
    "    if pad_b > 0:\n",
    "        img_r = np.concatenate([img_r, np.zeros([pad_b, new_w, 3], np.uint8)], 0)\n",
    "        masks_r = np.concatenate([masks_r, np.zeros([masks_r.shape[0], pad_b, new_w], np.uint8)], 1)\n",
    "    if pad_r > 0:\n",
    "        img_r = np.concatenate([img_r, np.zeros([target, pad_r, 3], np.uint8)], 1)\n",
    "        masks_r = np.concatenate([masks_r, np.zeros([masks_r.shape[0], target, pad_r], np.uint8)], 2)\n",
    "    return img_r, masks_r\n",
    "\n",
    "def sample_points_per_instance(masks: np.ndarray, k: int = 2, rng: np.random.Generator = None):\n",
    "    \"\"\"\n",
    "    For each non-empty instance mask, pick up to k random interior pixels.\n",
    "    Return:\n",
    "      pts_all: (P, 1, 2) int [[x,y]] per prompt\n",
    "      gt_idx:  (P,) int -> which mask each prompt should learn\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    PTS, IDX = [], []\n",
    "    for i, m in enumerate(masks):\n",
    "        ys, xs = np.nonzero(m)\n",
    "        if len(xs) == 0:\n",
    "            continue\n",
    "        sel = rng.choice(len(xs), size=min(k, len(xs)), replace=(len(xs) < k))\n",
    "        for s in np.atleast_1d(sel):\n",
    "            PTS.append([[int(xs[s]), int(ys[s])]])  # [[x,y]]\n",
    "            IDX.append(i)\n",
    "    if not PTS:\n",
    "        return np.zeros((0,1,2), np.int64), np.zeros((0,), np.int64)\n",
    "    return np.asarray(PTS, np.int64), np.asarray(IDX, np.int64)\n",
    "\n",
    "# --------------- Data list -------------------\n",
    "pairs = collect_pairs(TRAIN_DIR)  # unchanged\n",
    "\n",
    "# --------------- Train loop -------------\n",
    "mean_iou = 0.0\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "for itr in range(ITERS):\n",
    "    # minibatch of images; we will process images one-by-one (set_image per image)\n",
    "    batch_pairs = [pairs[rng.integers(len(pairs))] for _ in range(BATCH_SIZE)]\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_iou  = 0.0\n",
    "    total_prompts = 0\n",
    "\n",
    "    for (img_fp, msk_fp) in batch_pairs:\n",
    "        # --- load & prep data ---\n",
    "        img, masks_stack = load_pair(img_fp, msk_fp)            # (H,W,3), (N,H,W)\n",
    "        img = increase_contrast_pil(img, factor=5)\n",
    "        img, masks_stack = resize_and_pad_square(img, masks_stack, target=TARGET_SIZE)\n",
    "\n",
    "        # sample multiple seeds per instance (e.g., k=2)\n",
    "        pts_all, gt_idx = sample_points_per_instance(masks_stack, k=2, rng=rng)\n",
    "        if pts_all.shape[0] == 0:\n",
    "            continue  # no instances\n",
    "\n",
    "        # --- forward ---\n",
    "        use_amp = (DEVICE == \"cuda\")\n",
    "        with (torch.autocast(\"cuda\", dtype=torch.bfloat16) if use_amp else nullcontext()):\n",
    "            predictor.set_image(img)  # one image encoding\n",
    "\n",
    "            # prompts for P seeds\n",
    "            # shapes expected by predictor internals: (P,1,2) and (P,1)\n",
    "            labels_np = np.ones((pts_all.shape[0], 1), dtype=np.int64)  # all positive clicks\n",
    "            mask_input, unnorm_coords, labels_t, _ = predictor._prep_prompts(\n",
    "                pts_all, labels_np, box=None, mask_logits=None, normalize_coords=True\n",
    "            )\n",
    "\n",
    "            sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(\n",
    "                points=(unnorm_coords, labels_t), boxes=None, masks=None\n",
    "            )\n",
    "\n",
    "            # decode a SINGLE proposal per prompt (easier loss)\n",
    "            low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n",
    "                image_embeddings=predictor._features[\"image_embed\"],\n",
    "                image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "                sparse_prompt_embeddings=sparse_embeddings,   # (P, C)\n",
    "                dense_prompt_embeddings=dense_embeddings,     # (P, C, H', W')\n",
    "                multimask_output=False,                       # <â€” one mask per seed\n",
    "                repeat_image=True,\n",
    "                high_res_features=predictor._features[\"high_res_feats\"],\n",
    "            )\n",
    "            prd_masks = predictor._transforms.postprocess_masks(\n",
    "                low_res_masks, predictor._orig_hw[-1]\n",
    "            )  # (P,1,H,W)\n",
    "\n",
    "            # --- losses across all prompts for THIS image ---\n",
    "            P = prd_masks.shape[0]\n",
    "            gt = torch.from_numpy(masks_stack[gt_idx].astype(np.float32)).to(DEVICE)  # (P,H,W)\n",
    "            pred_prob = torch.sigmoid(prd_masks[:, 0])                                # (P,H,W)\n",
    "\n",
    "            # BCE loss\n",
    "            eps = 1e-5\n",
    "            seg_loss = (-gt * torch.log(pred_prob + eps) - (1 - gt) * torch.log(1 - pred_prob + eps)).mean()\n",
    "\n",
    "            # IoU calibration (pred score vs hard IoU of binarized pred)\n",
    "            pred_bin = (pred_prob > 0.5).float()\n",
    "            inter = (gt * pred_bin).sum((1, 2))\n",
    "            union = gt.sum((1, 2)) + pred_bin.sum((1, 2)) - inter + 1e-6\n",
    "            iou = inter / union\n",
    "            score_loss = torch.abs(prd_scores[:, 0].to(DEVICE) - iou).mean()\n",
    "\n",
    "            loss = seg_loss + 0.05 * score_loss\n",
    "\n",
    "        predictor.model.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += float(loss.detach().cpu())\n",
    "        total_iou  += float(iou.detach().cpu().mean())\n",
    "        total_prompts += P\n",
    "\n",
    "    if total_prompts == 0:\n",
    "        continue\n",
    "\n",
    "    mean_iou = 0.99 * mean_iou + 0.01 * (total_iou / max(1, len(batch_pairs)))\n",
    "\n",
    "    if itr % 10 == 0:\n",
    "        print(f\"step {itr:06d} | loss {total_loss/max(1,len(batch_pairs)):.4f} | mean IoU {mean_iou:.4f}\")\n",
    "\n",
    "    if itr % SAVE_EVERY == 0 and itr > 0:\n",
    "        torch.save({\"model\": predictor.model.state_dict()}, OUT_WEIGHTS)\n",
    "\n",
    "# final save\n",
    "torch.save({\"model\": predictor.model.state_dict()}, OUT_WEIGHTS)\n",
    "print(\"Saved:\", OUT_WEIGHTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5902adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from contextlib import nullcontext\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "def increase_contrast_pil(img_in, factor=5):\n",
    "    img_pil = img_in if isinstance(img_in, Image.Image) else Image.fromarray(img_in)\n",
    "    return np.array(ImageEnhance.Contrast(img_pil.convert(\"RGB\")).enhance(factor), dtype=np.uint8)\n",
    "\n",
    "def touches_border(m):\n",
    "    # m is a boolean/uint8 mask HxW\n",
    "    return m[0,:].any() or m[-1,:].any() or m[:,0].any() or m[:,-1].any()\n",
    "\n",
    "def filter_background_like(anns, max_area_frac=0.6, drop_edge_touch=False, max_bbox_cover=0.98):\n",
    "    out = []\n",
    "    Aimg = H * W\n",
    "    for ann in anns:\n",
    "        m = ann[\"segmentation\"]           # HxW bool/uint8\n",
    "        area = int(ann.get(\"area\", m.sum()))\n",
    "        x, y, w, h = ann.get(\"bbox\", [0,0, W, H])  # xywh in SAM\n",
    "        # 1) remove masks that are too large\n",
    "        if area > max_area_frac * Aimg:\n",
    "            continue\n",
    "        # 2) remove masks whose bbox nearly covers the image\n",
    "        if (w / W) > max_bbox_cover or (h / H) > max_bbox_cover:\n",
    "            continue\n",
    "        # 3) optionally remove masks touching the image border\n",
    "        if drop_edge_touch and touches_border(m):\n",
    "            continue\n",
    "        out.append(ann)\n",
    "    return out\n",
    "\n",
    "def show_anns(anns, borders=True):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    H, W = anns[0][\"segmentation\"].shape[:2]\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    # base: do NOT overwrite after drawing masks\n",
    "    # (call plt.imshow(image_np) BEFORE calling show_anns)\n",
    "\n",
    "    # build a single RGBA overlay\n",
    "    overlay = np.zeros((H, W, 4), dtype=np.float32)\n",
    "    for ann in sorted(anns, key=lambda x: x.get(\"area\", 0), reverse=True):\n",
    "        m = ann[\"segmentation\"].astype(bool)\n",
    "        color = np.random.rand(3)\n",
    "        rgba = np.r_[color, [0.5]]  # alpha 0.5\n",
    "        overlay[m] = rgba\n",
    "\n",
    "        if borders:\n",
    "            import cv2\n",
    "            cnts, _ = cv2.findContours(m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "            cnts = [cv2.approxPolyDP(c, epsilon=0.01, closed=True) for c in cnts]\n",
    "            cv2.drawContours(overlay, cnts, -1, (0, 0, 1, 0.4), thickness=1)\n",
    "\n",
    "    ax.imshow(overlay)  # draw on top\n",
    "\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8630e58a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SAM2AutomaticMaskGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m sam2 \u001b[38;5;241m=\u001b[39m build_sam2(cfg, ckpt, device\u001b[38;5;241m=\u001b[39mdevice, apply_postprocessing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# must be false otherwise cuda throws a paddy\u001b[39;00m\n\u001b[1;32m      9\u001b[0m predictor \u001b[38;5;241m=\u001b[39m SAM2ImagePredictor(sam2)\n\u001b[0;32m---> 11\u001b[0m mask_generator \u001b[38;5;241m=\u001b[39m \u001b[43mSAM2AutomaticMaskGenerator\u001b[49m(\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m=\u001b[39msam2,\n\u001b[1;32m     13\u001b[0m     pred_iou_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m image_pil \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m image_np  \u001b[38;5;241m=\u001b[39m increase_contrast_pil(image_pil, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)   \u001b[38;5;66;03m# HWC uint8\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SAM2AutomaticMaskGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "ckpt = \"/Users/sambra/Documents/GitHub/sam2_clone/checkpoints/sam2.1_hiera_l_finetuned.pt\"\n",
    "cfg  = \"/configs/sam2.1/sam2.1_hiera_t.yaml\"\n",
    "\n",
    "img_path = \"/Users/sambra/Desktop/training_images/MeOH_plate_A2-5945.tif\"  # change this to your file\n",
    "img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "# turn off the CUDA-only post-processing:\n",
    "sam2 = build_sam2(cfg, ckpt, device=device, apply_postprocessing=False) # must be false otherwise cuda throws a paddy\n",
    "predictor = SAM2ImagePredictor(sam2)\n",
    "\n",
    "mask_generator = SAM2AutomaticMaskGenerator(\n",
    "    model=sam2,\n",
    "    pred_iou_thresh=0.2,\n",
    ")\n",
    "\n",
    "image_pil = Image.open(img_path).convert(\"RGB\")\n",
    "image_np  = increase_contrast_pil(image_pil, factor=5)   # HWC uint8\n",
    "\n",
    "masks = mask_generator.generate(image_np)\n",
    "\n",
    "H, W = image_np.shape[:2]\n",
    "masks = filter_background_like(masks, max_area_frac=0.6, drop_edge_touch=True)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(image_np)     # base\n",
    "show_anns(masks)         # overlays on top\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"raw proposals:\", len(masks))\n",
    "print(\"after filter:\", len(masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = \"/Users/sambra/Documents/GitHub/sam2_clone/checkpoints/sam2.1_hiera_tiny.pt\"\n",
    "cfg  = \"/configs/sam2.1/sam2.1_hiera_t.yaml\"\n",
    "\n",
    "img_path = \"/Users/sambra/Desktop/training_images/MeOH_plate_A2-5945.tif\"  # change this to your file\n",
    "img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "# turn off the CUDA-only post-processing:\n",
    "sam2 = build_sam2(cfg, ckpt, device=device, apply_postprocessing=False) # must be false otherwise cuda throws a paddy\n",
    "predictor = SAM2ImagePredictor(sam2)\n",
    "\n",
    "mask_generator = SAM2AutomaticMaskGenerator(\n",
    "    model=sam2,\n",
    "    pred_iou_thresh=0.2,\n",
    ")\n",
    "\n",
    "image_pil = Image.open(img_path).convert(\"RGB\")\n",
    "image_np  = increase_contrast_pil(image_pil, factor=5)   # HWC uint8\n",
    "\n",
    "masks = mask_generator.generate(image_np)\n",
    "\n",
    "H, W = image_np.shape[:2]\n",
    "masks = filter_background_like(masks, max_area_frac=0.6, drop_edge_touch=True)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(image_np)     # base\n",
    "show_anns(masks)         # overlays on top\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"raw proposals:\", len(masks))\n",
    "print(\"after filter:\", len(masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1bc460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd050442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
