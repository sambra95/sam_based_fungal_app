{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8294583b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a9b5b19d7d4b95b7dc8f08409e9947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Upload one image, then click Classify:</b>'), FileUpload(value=(), accept='imageâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Single-image classification in a notebook (DenseNet121-based) ---\n",
    "\n",
    "import io\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Optional: quieter TF logs\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "# Instantiate the classifier\n",
    "classifier = load_model(\"classifier.h5\")\n",
    "\n",
    "def prepare_image_for_model(file_bytes, target_size=(64, 64)):\n",
    "    \"\"\"Load, RGB-convert, resize, and preprocess for DenseNet121.\"\"\"\n",
    "    img = Image.open(io.BytesIO(file_bytes)).convert(\"RGB\")\n",
    "    img_resized = img.resize(target_size, Image.BILINEAR)\n",
    "    arr = np.asarray(img_resized, dtype=np.float32)\n",
    "    arr = preprocess_input(arr)          # ImageNet-style normalization\n",
    "    batch = np.expand_dims(arr, axis=0)  # Add batch dimension\n",
    "    return img, batch  # return original PIL image for display, and preprocessed batch\n",
    "\n",
    "def classify_uploaded_image(file_bytes):\n",
    "    img_display, batch = prepare_image_for_model(file_bytes)\n",
    "    preds = classifier.predict(batch, verbose=0)[0]  # shape (2,)\n",
    "    pred_idx = int(np.argmax(preds))\n",
    "    pred_label = CLASS_NAMES[pred_idx]\n",
    "    return img_display, pred_label, preds\n",
    "\n",
    "# ---- Simple notebook UI with ipywidgets ----\n",
    "uploader = widgets.FileUpload(accept='image/*', multiple=False)\n",
    "run_btn = widgets.Button(description='Classify Image', button_style='primary')\n",
    "out = widgets.Output()\n",
    "\n",
    "def on_click_run(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        if not uploader.value:\n",
    "            print(\"Please upload one image first.\")\n",
    "            return\n",
    "\n",
    "        files = uploader.value\n",
    "        # Handle ipywidgets versions: dict (old), tuple/list (new)\n",
    "        if isinstance(files, dict):\n",
    "            file_info = next(iter(files.values()))\n",
    "        elif isinstance(files, (tuple, list)):\n",
    "            file_info = files[0]\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported FileUpload.value type: {type(files)}\")\n",
    "\n",
    "        file_bytes = file_info[\"content\"]\n",
    "\n",
    "        img, label, probs = classify_uploaded_image(file_bytes)\n",
    "\n",
    "        print(\"Prediction:\", label)\n",
    "        print(f\"Probabilities -> {CLASS_NAMES[0]}: {probs[0]:.4f}, {CLASS_NAMES[1]}: {probs[1]:.4f}\")\n",
    "        display(img)\n",
    "\n",
    "run_btn.on_click(on_click_run)\n",
    "\n",
    "display(widgets.VBox([widgets.HTML(\"<b>Upload one image, then click Classify:</b>\"),\n",
    "                       uploader, run_btn, out]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a173775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(\"classifier.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc8152a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire model (architecture + weights)\n",
    "classifier = load_model(\"/Users/sambra/Documents/GitHub/sam_based_fungal_app/classifier.keras\")\n",
    "path_input = \"/Users/sambra/Documents/GitHub/sporecounting/Segmented_Images/original/Wt 8h OH RB2_C00_ORG_ (1)_7.png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bbb6624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Germinated\n",
      "Ungerminated: 0.0001\n",
      "Germinated: 0.9999\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD5/oorVsdFku0LyF4hxt+XORQBlVah067uIhJFFuQ9DuA/rXU2+mW8MKI8UblRgs0YyasrHFGoVFRVHYDAoA5H+yL7/nh/4+v+NUmUo5Vhgg4Iru8KfSq8lnbEkmCIk8k7BQBxdFaup6aY3aWEFlYklVXAUVlUAX9JtftF8geItFzk446V2CKsUQUAAAAAVR0azksrVo5SpYuW+U+wqxcTYGBQAksrbsA00Pxz1qFSW5p4GaAJQfSgknqaaOBUTyEGgBJgCCp5B4Nc/qNksP75NoQkAKO3Fbsj5UGq7wR3KBJRlQc9cUAaNrdRXVuZIW3KGxyMc1XmJL1j6RqS23+jybVjZixcnpx/9atiXDqGXkEZBFADo/u1IOKprIYzz+tTC4U8nFAFnPFVpSM017kY4Iqu0hY8UAOLZbFQ3lw9pbLJGFJLY+b8amVcAu3AHJrE1CYS3TbJC0fGOeOlAFSrcGoTxSIWkd41/gzwR6VUooA6ZLi2mhWQyxIWGdpYZFOWFZF3RuGU9CvIrl6tQ6hdW8Qjil2oOg2g/wBKANt1ijbbJMinrhjimvPbQws4kicqMhQwyawZ7iW5kDzNuYDGcAcfhUVAFqa+mkdysjqjfwZ6CqtFFAH/2Q==",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Single-image classification from a file path (DenseNet121-based) ---\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# ==== Set this ====\n",
    "IMAGE_PATH = \"/Users/sambra/Documents/GitHub/sporecounting/Segmented_Images/original/Wt 8h OH RB2_C00_ORG_ (1)_7.png\"   # <-- put your image path here\n",
    "MODEL_PATH = \"/Users/sambra/Documents/GitHub/sam_based_fungal_app/classifier.keras\"       # change if needed\n",
    "CLASS_NAMES = [\"Ungerminated\", \"Germinated\"]  # adjust to your classes\n",
    "\n",
    "# Optional: quieter TF logs\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "# Load model\n",
    "classifier = load_model(MODEL_PATH)\n",
    "\n",
    "def prepare_image_from_path(path: str, target_size=(64, 64)):\n",
    "    \"\"\"Load file path -> RGB -> resize -> preprocess -> return (PIL image, batch).\"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Image path not found: {p}\")\n",
    "\n",
    "    img = Image.open(p).convert(\"RGB\")\n",
    "    img_resized = img.resize(target_size, Image.BILINEAR)\n",
    "    arr = np.asarray(img_resized, dtype=np.float32)\n",
    "    arr = preprocess_input(arr)                    # ImageNet-style normalization\n",
    "    batch = np.expand_dims(arr, axis=0)           # (1, H, W, 3)\n",
    "    return img, batch\n",
    "\n",
    "def classify_image_path(path: str):\n",
    "    img_display, batch = prepare_image_from_path(path)\n",
    "    preds = classifier.predict(batch, verbose=0)[0]   # shape (num_classes,)\n",
    "    pred_idx = int(np.argmax(preds))\n",
    "    pred_label = CLASS_NAMES[pred_idx]\n",
    "    return img_display, pred_label, preds\n",
    "\n",
    "# Run classification\n",
    "img, label, probs = classify_image_path(IMAGE_PATH)\n",
    "print(\"Prediction:\", label)\n",
    "for i, name in enumerate(CLASS_NAMES):\n",
    "    print(f\"{name}: {probs[i]:.4f}\")\n",
    "display(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be50249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
